{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Malla_Shweta_Lab2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "USb91M-NhA0P",
        "outputId": "479e81c5-94a4-49e0-deed-321222566903",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import files,drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqDMesW0lKLC"
      },
      "source": [
        "# creating a CNN model for handwritiing digit recognition problem \n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0LXDj3Xm7w9"
      },
      "source": [
        "#to enable GPU \n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LSRry8VoTja"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__() \n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "    self.pool= nn.MaxPool2d(2,2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.fc1 = nn.Linear(16*4*4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.pool(F.relu(self.conv1(x)))\n",
        "    x = self.pool(F.relu(self.conv2(x)))\n",
        "    x = x.view(-1, self.num_flat_features(x))   #to flatten, conevert x to batches\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = F.log_softmax(self.fc3(x))\n",
        "    #x = F.dropout(x, training=self.training)\n",
        "    \n",
        "    return x\n",
        "  #input image =1x28x28, white and black image \n",
        "#here we define  first conv layer (input channel: 1, output_channel:6, filter_size: 5*5)\n",
        "#define pooling layer (2x2)\n",
        "#second conv layer (input channel: 6, output_channel:16, filter_size: 5*5)\n",
        "#define a sub fully connected feedforward network \n",
        "# hidden size (1):120\n",
        "#hidden size (2): 84\n",
        "#output size: 10 \n",
        "\n",
        "  def num_flat_features(self,x):\n",
        "    size= x.size()[1:]\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *= s\n",
        "    return num_features\n"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6KbVO4soUF-"
      },
      "source": [
        "net = Net().to(device)"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWxfPhVsuffo",
        "outputId": "0cff47a8-d5da-48ed-948d-1f9a411e0f45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlyj3g7Qu5r_"
      },
      "source": [
        "#we will create a customized the dataset , we will save as a dictionary, eg: image of 2 will have label 2 \n",
        "import os\n",
        "import glob\n",
        "import numpy as np  #for us to create the dictionary \n",
        "from skimage import io #it can convert image to numpy array \n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VY8Y3omDra5"
      },
      "source": [
        "class MNISTDataset(Dataset):\n",
        "  def __init__(self, dir, transform=None): # we are storing it in dir and we are not transforming the instances\n",
        "    self.dir = dir\n",
        "    self.transform = transform\n",
        "\n",
        "  def __len__(self):  #instances in dataset \n",
        "    files = glob.glob(self.dir+'/*.jpg')[:100] # we are reading only 100 images for faster compilation\n",
        "    return len(files)   #number of files\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()       #convert from tesor to list \n",
        "    all_files = glob.glob(self.dir+'/*.jpg')[:100] #returns a list of filenames\n",
        "    img_fname = os.path.join(self.dir, all_files[idx]) #obtain an absolute path to file with index \n",
        "    image = io.imread(img_fname) # numpy array for image\n",
        "    digit = int(self.dir.split('/')[-1].strip()) \n",
        "    label = np.array(digit)    #\n",
        "\n",
        "    instance = {'image':image,'label':label} #dictinary with 2 key:value pairs , image:label\n",
        "\n",
        "    if self.transform:\n",
        "      instance = self.transform(instance)\n",
        "\n",
        "    return instance\n",
        "  \n"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-LOeNOZGCBR"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy3tcLjVFf2I"
      },
      "source": [
        "#create a customized transformation for each instance in dataset \n",
        "#We will try to rescale the image here\n",
        "from skimage import transform\n",
        "from torchvision import transforms, utils"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ino57UzgL0p1"
      },
      "source": [
        "class Rescale(object):\n",
        "   def __init__(self, output_size):\n",
        "     assert isinstance(output_size, (int, tuple))   #check if tuple \n",
        "     self.output_size = output_size\n",
        "\n",
        "   def __call__(self, sample):      #we provide the new size (after rescaling), sample is a dictionary \n",
        "     image, label = sample['image'], sample['label']\n",
        "\n",
        "     h, w = image.shape[-2:]    #height,width(rows,cols)\n",
        "     if isinstance(self.output_size, int):    #if innt else tuple\n",
        "       if h > w:\n",
        "         new_h, new_w = self.output_size*h/w, self.output_size\n",
        "       else:\n",
        "         new_h, new_w = self.output_size, self.output_size*w/h\n",
        "     else:\n",
        "       new_h, new_w = self.output_size\n",
        "   \n",
        "     new_h, new_w = int(new_h), int(new_w)\n",
        "     new_image = transform.resize(image, (new_h, new_w)) \n",
        "   \n",
        "       #this is the new sample \n",
        "     return {'image': new_image, 'label':label}"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzj5f-RN-y0"
      },
      "source": [
        "#convert every instance to tensor , we will just overwrite call not the initalize function\n",
        "class ToTensor(object):\n",
        "   def __call__(self, sample):\n",
        "     image, label = sample['image'], sample['label']\n",
        "     image = image.reshape((1,image.shape[0],image.shape[1]))\n",
        "     return {'image':torch.from_numpy(image) ,'label': torch.from_numpy(label)} #image: convert to tensor from numpy"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9alaEECrPc-e"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms, utils"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "damaUtQIP0Mv",
        "outputId": "57404d15-a243-4845-dbda-25c88aad2203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#create train/val dataloader\n",
        "\n",
        "batch_size = 32\n",
        "list_datasets = []\n",
        "\n",
        "for i in range(10):     #we are combining 10 folders into one dataset (training)\n",
        "  cur_ds = MNISTDataset('/content/drive/My Drive/MNIST/trainingset/'+str(i), transform=transforms.Compose([Rescale(28), ToTensor()]))\n",
        "  list_datasets.append(cur_ds)\n",
        "\n",
        "dataset = torch.utils.data.ConcatDataset(list_datasets) #this will have all instances with transformation\n",
        "print(len(dataset))"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkOdsUH7QTnG"
      },
      "source": [
        "train_size = int(len(dataset)*0.7)\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset,[train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset,batch_size,shuffle=True,num_workers=1)\n",
        "val_dataloader = DataLoader(val_dataset,batch_size,shuffle=True,num_workers=1)"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHXiYU4iR_hZ",
        "outputId": "0d2f2262-8092-46d0-d414-a1736b374037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Train the model and valiadation \n",
        "\n",
        "epoch = 5\n",
        "learning_rate =1e-3\n",
        "optimizer = optim.Adam(net.parameters(),lr=learning_rate, weight_decay=1e-5)\n",
        "criterion= nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epoch):\n",
        "  net.train()\n",
        "  running_loss =0.0\n",
        "  \n",
        "  for batch_idx, batch in enumerate(train_dataloader):\n",
        "    inputs, targets = batch['image'].to(device,dtype=torch.float), batch['label'].to(device,dtype= torch.long)\n",
        "\n",
        "    predicted_outputs = net(inputs)\n",
        "    optimizer.zero_grad()\n",
        "   \n",
        "    loss = criterion(predicted_outputs,targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss +=loss.item()\n",
        "    if (batch_idx+1)%10==0:\n",
        "      print('epoch %d, batch : %d, training_loss: %.3f'%(epoch+1, batch_idx+1, running_loss/10))\n",
        "      running_loss=0.0\n",
        "\n",
        "  net.eval()\n",
        "\n",
        "  correct = [0.0]*10\n",
        "  total= [0.0]*10 \n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(val_dataloader):\n",
        "      inputs, labels = batch['image'].to(device,dtype=torch.float), batch['label'].to(device,dtype= torch.long)\n",
        "      predicted_outputs = net(inputs)\n",
        "\n",
        "      _,predicted_labels = torch.max(predicted_outputs,1)\n",
        "      c= (predicted_labels == labels)\n",
        "\n",
        "      for i in range(len(labels)):\n",
        "        label = labels[i]   #true labels \n",
        "        correct[label] += c[i].item()     #to check if it correct\n",
        "        total[label] += 1   #total for that digit\n",
        "  for i in range(10):\n",
        "    print('\\t Validation accuracy for digit %d: %.2f'%(i, 100*correct[i]/total[i]))\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1, batch : 10, training_loss: 0.262\n",
            "epoch 1, batch : 20, training_loss: 0.323\n",
            "\t Validation accuracy for digit 0: 89.29\n",
            "\t Validation accuracy for digit 1: 96.30\n",
            "\t Validation accuracy for digit 2: 88.46\n",
            "\t Validation accuracy for digit 3: 93.55\n",
            "\t Validation accuracy for digit 4: 93.33\n",
            "\t Validation accuracy for digit 5: 91.43\n",
            "\t Validation accuracy for digit 6: 93.55\n",
            "\t Validation accuracy for digit 7: 82.35\n",
            "\t Validation accuracy for digit 8: 56.67\n",
            "\t Validation accuracy for digit 9: 89.29\n",
            "epoch 2, batch : 10, training_loss: 0.191\n",
            "epoch 2, batch : 20, training_loss: 0.207\n",
            "\t Validation accuracy for digit 0: 89.29\n",
            "\t Validation accuracy for digit 1: 92.59\n",
            "\t Validation accuracy for digit 2: 80.77\n",
            "\t Validation accuracy for digit 3: 93.55\n",
            "\t Validation accuracy for digit 4: 93.33\n",
            "\t Validation accuracy for digit 5: 91.43\n",
            "\t Validation accuracy for digit 6: 90.32\n",
            "\t Validation accuracy for digit 7: 91.18\n",
            "\t Validation accuracy for digit 8: 80.00\n",
            "\t Validation accuracy for digit 9: 75.00\n",
            "epoch 3, batch : 10, training_loss: 0.130\n",
            "epoch 3, batch : 20, training_loss: 0.204\n",
            "\t Validation accuracy for digit 0: 89.29\n",
            "\t Validation accuracy for digit 1: 100.00\n",
            "\t Validation accuracy for digit 2: 76.92\n",
            "\t Validation accuracy for digit 3: 83.87\n",
            "\t Validation accuracy for digit 4: 100.00\n",
            "\t Validation accuracy for digit 5: 91.43\n",
            "\t Validation accuracy for digit 6: 83.87\n",
            "\t Validation accuracy for digit 7: 82.35\n",
            "\t Validation accuracy for digit 8: 80.00\n",
            "\t Validation accuracy for digit 9: 85.71\n",
            "epoch 4, batch : 10, training_loss: 0.143\n",
            "epoch 4, batch : 20, training_loss: 0.178\n",
            "\t Validation accuracy for digit 0: 89.29\n",
            "\t Validation accuracy for digit 1: 100.00\n",
            "\t Validation accuracy for digit 2: 92.31\n",
            "\t Validation accuracy for digit 3: 93.55\n",
            "\t Validation accuracy for digit 4: 80.00\n",
            "\t Validation accuracy for digit 5: 94.29\n",
            "\t Validation accuracy for digit 6: 93.55\n",
            "\t Validation accuracy for digit 7: 88.24\n",
            "\t Validation accuracy for digit 8: 70.00\n",
            "\t Validation accuracy for digit 9: 92.86\n",
            "epoch 5, batch : 10, training_loss: 0.133\n",
            "epoch 5, batch : 20, training_loss: 0.153\n",
            "\t Validation accuracy for digit 0: 89.29\n",
            "\t Validation accuracy for digit 1: 88.89\n",
            "\t Validation accuracy for digit 2: 92.31\n",
            "\t Validation accuracy for digit 3: 93.55\n",
            "\t Validation accuracy for digit 4: 96.67\n",
            "\t Validation accuracy for digit 5: 91.43\n",
            "\t Validation accuracy for digit 6: 93.55\n",
            "\t Validation accuracy for digit 7: 82.35\n",
            "\t Validation accuracy for digit 8: 90.00\n",
            "\t Validation accuracy for digit 9: 92.86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE7eXyvQVBmT"
      },
      "source": [
        "#x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    #x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srPhjwifeEdv"
      },
      "source": [
        ""
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}